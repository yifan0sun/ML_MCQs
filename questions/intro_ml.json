[
  {
    "id": 1,
    "question": "Which of the following is an example of unsupervised learning?",
    "options": [
      "K-Means clustering",
      "Linear regression",
      "Decision tree classification",
      "Neural network regression"
    ],
    "correct": [0],
    "explanation": "K-Means is a classic example of unsupervised learning, which finds patterns in unlabeled data.",
    "level": "basic"
  },
  {
    "id": 2,
    "question": "In semi-supervised learning, what kind of data is used?",
    "options": [
      "Only labeled data",
      "Only unlabeled data",
      "Both labeled and unlabeled data",
      "Data with explicit constraints"
    ],
    "correct": [2],
    "explanation": "Semi-supervised learning leverages both labeled and unlabeled data to improve model performance.",
    "level": "basic"
  },
  {
  "id": 3,
  "question": "Select all which are examples of semi-supervised learning.",
  "options": [
    "Using labeled and unlabeled data to train a neural network for image classification",
    "Training a model using weak labels generated by another DNN combined with a small set of human-labeled data",
    "Exact classification labels are not available, so the model uses metadata, in the form of pairwise comparisons, to obtain partial information.",
    "Using hierarchical labels for a large dataset to infer missing labels for specific categories"
  ],
  "correct": [0, 1, 2, 3],
  "explanation": "Semi-supervised learning includes leveraging both labeled and unlabeled data. Weak labels, such as those generated by another DNN or inferred using comparisons, are valid examples. Hierarchical labeling is also semi-supervised when using coarse-grained labels to infer fine-grained ones. These techniques extend semi-supervised learning's reach by introducing partially accurate labels to guide the learning process.",
  "level": "advanced"
},
  {
    "id": 4,
    "question": "What is the key difference between generative and discriminative models?",
    "options": [
      "Generative models model the joint probability distribution P(X, Y), while discriminative models model P(Y|X)",
      "Discriminative models can only be used for classification tasks",
      "Generative models always require labeled data",
      "Discriminative models focus on unsupervised learning"
    ],
    "correct": [0],
  "explanation": "Generative models learn the joint probability P(X, Y), which allows them to model the data distribution  P(X)  and generate new samples. In contrast, discriminative models focus on the conditional probability P(Y|X), which requires less granularity as it directly models the decision boundary for classification or regression tasks without needing to fully understand P(X).",
    "level": "advanced"
  },
  {
  "id": 5,
  "question": "How do generative and discriminative models differ in terms of data requirements?",
  "options": [
    "Discriminative models typically need more data to perform well as they do not model the input distribution P(X)",
    "Generative models often require more data to train effectively because they learn the full joint distribution P(X, Y)",
    "Generative models need labeled data only, while discriminative models can use unlabeled data",
    "Both generative and discriminative models require the same amount of data under all conditions"
  ],
  "correct": [1],
  "explanation": "Generative models learn the joint distribution P(X, Y), which is a more complex task than modeling P(Y|X) alone. This often necessitates larger datasets to avoid overfitting and to accurately capture the data distribution.",
  "level": "advanced"
},
{
  "id": 6,
  "question": "What is a key advantage of generative models over discriminative models in specific tasks?",
  "options": [
    "Discriminative models can generate new samples, while generative models cannot",
    "Discriminative models are inherently more explainable than generative models",
    "Generative models can generate synthetic data and perform unsupervised learning, while discriminative models are limited to prediction tasks",
    "Generative models always achieve higher accuracy than discriminative models on classification tasks"
  ],
  "correct": [2],
  "explanation": "Generative models excel in tasks where data generation or unsupervised learning is needed because they learn the joint distribution P(X, Y). Discriminative models, on the other hand, are better suited for tasks that focus purely on prediction, like classification or regression.",
  "level": "advanced"
},
{
  "id": 7,
  "question": "How do generative and discriminative models differ in terms of their focus during training?",
  "options": [
    "Generative models aim to model the entire data distribution holistically, while discriminative models focus only on learning the decision boundary between classes",
    "Discriminative models aim to model the data distribution holistically, while generative models focus only on specific decision boundaries",
    "Both generative and discriminative models aim to model the data distribution but differ in optimization techniques",
    "Generative models always outperform discriminative models in classification tasks because they focus on holistic modeling"
  ],
  "correct": [0],
  "explanation": "Generative models focus on the joint probability distribution P(X, Y), providing a holistic view of the data, including P(X). Discriminative models, in contrast, concentrate on learning P(Y|X), directly optimizing the decision boundary for tasks like classification or regression.",
  "level": "advanced"
},
  {
    "id": 8,
    "question": "How does Machine Learning (ML) differ from Artificial Intelligence (AI)?",
    "options": [
      "AI only involves neural networks, while ML does not",
	  "ML focuses on algorithms that learn from data, while AI includes broader goals like reasoning and problem-solving",
      "ML is a type of unsupervised learning, while AI includes supervised learning",
      "AI requires labeled data, but ML does not"
    ],
    "correct": [1],
    "explanation": "Machine Learning is a subset of AI that uses algorithms to learn patterns from data, while AI encompasses broader fields, including reasoning, decision-making, and path planning.",
    "level": "basic"
  },
  {
    "id": 9,
    "question": "What is the primary goal of supervised learning?",
    "options": [
      "To group similar data points into clusters",
      "To generate new samples from a learned distribution",
      "To maximize the entropy of predictions",
      "To learn a mapping from input features to output labels"
    ],
    "correct": [3],
    "explanation": "The goal of supervised learning is to use labeled training data to learn a function that maps input features to output labels, enabling predictions on new, unseen data.",
    "level": "basic"
  },
  {
    "id": 10,
    "question": "What is a commonly desired application-based goal achievable by unsupervised learning?",
    "options": [
      "To group similar data points into clusters",
      "To generate new samples from a learned distribution",
      "To maximize the entropy of predictions",
      "To learn a mapping from input features to output labels"
    ],
    "correct": [0],
    "explanation": "Unsupervised learning is commonly used to group data points into clusters, identifying patterns or similarities without the need for labeled data.",
    "level": "basic"
  },
  {
    "id": 11,
    "question": "What is one way of injecting diversity and avoiding modal collapse?",
    "options": [
      "To group similar data points into clusters",
      "To generate new samples from a learned distribution",
      "To maximize the entropy of predictions",
      "To learn a mapping from input features to output labels"
    ],
    "correct": [2],
    "explanation": "Maximizing the entropy of predictions is a technique used to encourage diversity in outputs and avoid modal collapse, particularly in generative models.",
    "level": "advanced"
  },
  {
    "id": 12,
    "question": "What is an important application of machine learning that does not have a clear set of labels but demonstrates the model's ability to generate meaningful data?",
    "options": [
      "To group similar data points into clusters",
      "To generate new samples from a learned distribution",
      "To maximize the entropy of predictions",
      "To learn a mapping from input features to output labels"
    ],
    "correct": [1],
    "explanation": "Generating new samples from a learned distribution is a key application of generative models, which can demonstrate a model's understanding of the underlying data structure without relying on explicit labels.",
    "level": "advanced"
  },
  {
    "id": 13,
    "question": "Which of the following tasks are examples of classification? (Multiple answers are allowed)",
    "options": [
      "Predicting whether an email is spam or not",
      "Identifying the species of a flower based on its features",
      "Estimating the price of a house based on its attributes",
      "Labeling pixels in an image as road, vehicle, or pedestrian"
    ],
    "correct": [0, 1, 3],
    "explanation": "Classification involves assigning a label to data points. Tasks like spam detection, species identification, and semantic segmentation are examples.",
    "level": "basic"
  },
  {
  "id": 14,
  "question": "Which of the following tasks are examples of regression? (Multiple answers are allowed)",
  "options": [
    "Predicting the temperature for the next day based on historical weather data",
    "Estimating the price of a house based on its attributes",
    "Classifying a set of emails as spam or not",
    "Forecasting the revenue of a company for the next quarter"
  ],
  "correct": [0, 1, 3],
  "explanation": "Regression involves predicting a continuous numerical value. Tasks like temperature prediction, house price estimation, and revenue forecasting are examples. Classifying emails as spam or not is an example of classification, not regression; it is classification.",
  "level": "basic"
},
  {
    "id": 15,
    "question": "What is the difference between regression and classification in supervised learning?",
    "options": [
      "Regression works only with labeled data, while classification works with unlabeled data",
      "Regression predicts continuous values, while classification predicts discrete labels",
      "Regression uses clustering algorithms, while classification uses neural networks",
      "Regression minimizes entropy, while classification minimizes variance"
    ],
    "correct": [1],
    "explanation": "Regression predicts continuous outputs (e.g., house prices), while classification predicts discrete categories (e.g., spam vs not spam).",
    "level": "basic"
  },
  {
    "id": 16,
    "question": "Which of the following models is commonly used to predict a continuous output variable?",
    "options": [
      "K-Means clustering",
      "Linear regression",
      "Naive Bayes classifier",
      "Support Vector Machine"
    ],
    "correct": [1],
    "explanation": "Linear regression is a supervised learning model used to predict continuous output variables.",
    "level": "basic"
  },
  {
    "id": 17,
    "question": "Which model is best suited for separating data into classes with a linear decision boundary?",
    "options": [
      "Support Vector Machine (SVM)",
      "K-Means clustering",
      "Principal Component Analysis (PCA)",
      "Random forest"
    ],
    "correct": [0],
    "explanation": "SVM is a supervised learning model that works well for classification with a linear decision boundary.",
    "level": "basic"
  },
{
  "id": 18,
  "question": "Which of the following supervised learning algorithms builds a probabilistic model but relies on a key assumption of feature independence to reduce sample complexity? (Multiple answers are allowed)",
  "options": [
    "K-Nearest Neighbors (KNN)",
    "K-Means clustering",
    "Random forest",
    "Naive Bayes classifier"
  ],
  "correct": [3],
  "explanation": "Naive Bayes builds a probabilistic model by assuming that features are conditionally independent given the class label. This simplification allows it to reduce sample complexity while still being effective for tasks like spam detection.",
  "level": "basic"
},
  {
    "id": 19,
    "question": "Which model is often used to handle large datasets with both categorical and numerical features?",
    "options": [
      "Gaussian Mixture Model (GMM)",
      "Support Vector Machine (SVM)",
      "K-Nearest Neighbors (KNN)",
      "Decision tree"
    ],
    "correct": [3],
    "explanation": "Decision trees are versatile and can handle mixed datasets efficiently.",
    "level": "basic"
  },
  {
    "id": 20,
    "question": "Which supervised learning model is commonly used for predicting binary outcomes?",
    "options": [
      "K-Means clustering",
      "Linear regression",
      "Gaussian Mixture Model (GMM)",
      "Logistic regression"
    ],
    "correct": [3],
    "explanation": "Logistic regression is used for predicting binary outcomes by modeling the probability of a target class.",
    "level": "basic"
  },
  {
    "id": 21,
    "question": "Which of the following models is used to group similar data points into clusters?",
    "options": [
      "Linear regression",
      "K-Means clustering",
      "Support Vector Machine",
      "Random forest"
    ],
    "correct": [1],
    "explanation": "K-Means clustering is an unsupervised learning model used to group similar data points into clusters.",
    "level": "basic"
  },
  {
    "id": 22,
    "question": "Which unsupervised learning model is often used to reduce the dimensionality of data?",
    "options": [
      "Logistic regression",
      "K-Nearest Neighbors (KNN)",
      "Principal Component Analysis (PCA)",
      "Random forest"
    ],
    "correct": [2],
    "explanation": "PCA is used in unsupervised learning to reduce dimensionality and identify hidden structures in data.",
    "level": "basic"
  },
{
  "id": 23,
  "question": "Market segmentation in customer analytics refers to dividing a broad customer base into smaller, more manageable groups based on shared characteristics. Which model can be best used for this task?",
  "options": [
    "Random forest",
    "Logistic regression",
    "K-Means clustering",
    "Support Vector Machine (SVM)"
  ],
  "correct": [2],
  "explanation": "K-Means clustering is commonly used for market segmentation because it groups customers into clusters based on similarities in their characteristics, enabling targeted strategies.",
  "level": "basic"
},
  {
    "id": 24,
    "question": "Which unsupervised learning model is used to estimate the density of data points?",
    "options": [
      "Decision tree",
      "Gaussian Mixture Model (GMM)",
      "K-Nearest Neighbors (KNN)",
      "Logistic regression"
    ],
    "correct": [1],
    "explanation": "GMM is an unsupervised learning model used for density estimation and clustering.",
    "level": "basic"
  }, 
  {
    "id": 25,
    "question": "Which of the following are examples of features in a dataset? (Multiple answers are allowed)",
    "options": [
      "The size of a house in square feet",
      "The color of a car",
      "The predicted price of a house",
      "The text of a product review"
    ],
    "correct": [0, 1, 3],
    "explanation": "Features are inputs to a model, such as numeric, categorical, or textual data describing an instance.",
    "level": "basic"
  },
  {
    "id": 26,
    "question": "What is overfitting in machine learning?",
    "options": [
      "A model achieves high accuracy on both training and test data",
      "A model's predictions are consistent across datasets",
      "A model performs well on training data but poorly on unseen data",
      "A model uses fewer features than necessary"
    ],
    "correct": [2],
    "explanation": "Overfitting occurs when a model learns noise or irrelevant details from training data, leading to poor generalization.",
    "level": "basic"
  },
{
  "id": 27,
  "question": "Overfitting is more likely to happen if (multiple answers may apply):",
  "options": [
    "The dataset is small",
    "The model has a lot of parameters",
    "The model has high complexity or low regularization (e.g., deep decision trees or KNN with K=1)",
    "The distribution of the training dataset is limited"
  ],
  "correct": [0, 1, 2,3],
  "explanation": "Overfitting occurs when a model learns noise or irrelevant details from the training data instead of general patterns. Small datasets provide fewer examples, increasing the risk of memorization. Models with many parameters or high complexity (such as deep decision trees or KNN with K=1) are prone to overfitting, as they adapt too closely to training data without generalizing. While model instability (e.g., optimization issues) may exacerbate overfitting, it is not a direct cause. Limited training data distribution increases the risk of overfitting because the model is exposed to only a narrow range of scenarios and may fail to generalize to diverse test data.",
  "level": "basic"
},
  {
    "id": 28,
    "question": "What is clustering in unsupervised learning?",
    "options": [
      "Assigning labels to data points based on a trained model",
      "Predicting a continuous output for input features",
      "Grouping data points into similar clusters based on their features",
      "Reducing the dimensionality of the data"
    ],
    "correct": [2],
    "explanation": "Clustering is an unsupervised learning technique that groups data points into clusters based on their similarities.",
    "level": "basic"
  },
  {
    "id": 29,
    "question": "Which of the following are examples of generative tasks? (Multiple answers are allowed)",
    "options": [
      "Generating realistic images",
      "Synthesizing text",
      "Classifying emails as spam or not",
      "Clustering similar news articles"
    ],
    "correct": [0, 1],
    "explanation": "Generative tasks involve creating new data samples, such as generating images or synthesizing text.",
    "level": "advanced"
  }, 
  {
    "id": 31,
    "question": "What is the primary goal of generalization in machine learning?",
    "options": [
      "To perform well on unseen data",
      "To achieve 100% accuracy on training data",
      "To minimize training time",
      "To maximize the number of features used"
    ],
    "correct": [0],
    "explanation": "Generalization is the ability of a model to perform well on unseen data by capturing the underlying patterns in the data.",
    "level": "basic"
  },
  {
    "id": 32,
    "question": "Which of the following are examples of generative tasks? (Multiple answers are allowed)",
    "options": [
      "Generating realistic images",
      "Synthesizing text",
      "Classifying emails as spam or not",
      "Clustering similar news articles"
    ],
    "correct": [0, 1],
    "explanation": "Generative tasks involve creating new data samples, such as generating images or synthesizing text.",
    "level": "advanced"
  },
  {
    "id": 33,
    "question": "Which of the following scenarios best describe semi-supervised learning? (Circle all that are true.)",
    "options": [
      "A dataset where there are no clear labels, but some label clues; e.g., user preferences are derived from comparisons like 'user prefers item A over item B'.",
      "A dataset consists entirely of labeled examples, with the model learning directly from these labels.",
      "The model is provided with a small labeled dataset, and the remaining large dataset is unlabeled, helping the model generalize better.",
      "The dataset is completely unlabeled, and the model clusters the data into different groups based on similarities."
    ],
    "correct": [0, 2],
    "explanation": "Semi-supervised learning leverages a combination of labeled and unlabeled data, where labeled examples provide supervision and unlabeled examples improve generalization.",
    "level": "basic"
  },
  {
    "id": 34,
    "question": "What is the primary purpose of PCA in dimensionality reduction?",
    "options": [
      "To increase the complexity of the dataset",
      "To reduce the dimensionality of the dataset while preserving as much variance as possible",
      "To cluster similar data points together",
      "To perform non-linear mapping of data"
    ],
    "correct": [1],
    "explanation": "Principal Component Analysis (PCA) reduces the dimensionality of the data by preserving as much variance as possible, making it easier to analyze.",
    "level": "basic"
  },
  {
    "id": 35,
    "question": "Why is training data critical in learning?",
    "options": [
      "It helps validate the final model.",
      "It provides the basis for feature selection.",
      "It is used to test the model.",
      "It allows the model to learn from examples."
    ],
    "correct": [3],
    "explanation": "Training data is essential because it provides examples from which the model learns to identify patterns and make predictions.",
    "level": "basic"
  },  
  {
    "id": 36,
    "question": "What is the main difference between supervised and unsupervised learning?",
    "options": [
      "Supervised learning requires a training set with labels, while unsupervised learning does not use labeled data.",
      "Unsupervised learning uses labeled data, while supervised learning does not.",
      "Supervised learning is used for clustering, while unsupervised learning is used for classification.",
      "There is no difference between supervised and unsupervised learning."
    ],
    "correct": [0],
    "explanation": "Supervised learning requires labeled data to train models, while unsupervised learning identifies patterns in unlabeled data.",
    "level": "basic"
  },
{
  "id": 37,
  "question": "What are common strategies for handling missing values in datasets? (Multiple answers are allowed)",
  "options": [
    "Imputing with the mean, median, or mode of the feature",
    "Removing rows or columns containing missing values",
    "Using machine learning models like KNN for imputation",
    "Assigning completely random values to the missing entries"
  ],
  "correct": [0, 1, 2],
  "explanation": "Common strategies for handling missing data include imputing with statistical measures (mean, median, or mode), removing rows or columns with missing entries, and using advanced methods like KNN-based imputation. Assigning random values is not a standard or recommended approach.",
  "level": "advanced"
}
  ]