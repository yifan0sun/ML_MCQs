[
  {
    "id": 1,
    "question": "What is the primary goal of recommender systems in machine learning?",
    "options": [
      "To classify different types of users",
      "To predict the next item a user might purchase or like",
      "To cluster users based on their preferences",
      "To reduce the dimensionality of data"
    ],
    "correct": [1],
    "explanation": "Recommender systems aim to predict a user's preferences and suggest items they are likely to interact with, such as movies, products, or content.",
    "level": "basic"
  },
  {
    "id": 2,
    "question": "In the context of recommender systems, what does matrix factorization aim to achieve?",
    "options": [
      "To cluster similar users together",
      "To decompose the user-item matrix into lower-dimensional user and item matrices",
      "To increase the number of features in the data",
      "To perform non-linear classification of user preferences"
    ],
    "correct": [1],
    "explanation": "Matrix factorization decomposes the user-item interaction matrix into lower-dimensional representations, allowing for more efficient prediction of user preferences.",
    "level": "basic"
  },
  {
    "id": 3,
    "question": "Which manifold learning technique is most commonly used to visualize data more effectively?",
    "options": [
      "t-SNE",
      "Isomap",
      "Locally linear embedding",
      "Matrix factorization"
    ],
    "correct": [0],
    "explanation": "t-SNE (t-Distributed Stochastic Neighbor Embedding) is widely used for visualizing high-dimensional data in a lower-dimensional space.",
    "level": "basic"
  },
  {
    "id": 4,
    "question": "What is the primary objective of an autoencoder in neural networks?",
    "options": [
      "To classify input data into predefined categories",
      "To learn a compressed, low-dimensional representation of the input data",
      "To increase the dimensionality of the input data",
      "To cluster the input data based on its features"
    ],
    "correct": [1],
    "explanation": "An autoencoder aims to learn a compressed representation (encoding) of input data while minimizing the reconstruction error.",
    "level": "basic"
  },

{
  "id": 5,
  "question": "What is the significance of the latent space in a Variational Autoencoder?",
  "options": [
    "It allows the generation of new samples by sampling from a probability distribution",
    "It stores compressed binary representations of the input data",
    "It ensures deterministic mappings from inputs to outputs",
    "It provides a direct method for classifying data"
  ],
  "correct": [0],
  "explanation": "The latent space in a VAE is modeled as a probability distribution, allowing new samples to be generated by sampling from the distribution.",
  "level": "advanced"
},
  {
    "id": 6,
    "question": "Which technique is used in recommender systems to predict user preferences based on collaborative filtering?",
    "options": [
      "Matrix Factorization",
      "t-SNE",
      "PCA",
      "Random Projections"
    ],
    "correct": [0],
    "explanation": "Matrix factorization is commonly used in collaborative filtering to decompose the user-item matrix.",
    "level": "basic"
  },
  {
    "id": 7,
    "question": "What is the main advantage of using random projections for dimensionality reduction?",
    "options": [
      "It guarantees a perfect reconstruction of the original data.",
      "It preserves distances between points with high probability.",
      "It requires labeled data.",
      "It only works for image data."
    ],
    "correct": [1],
    "explanation": "Random projections preserve distances between points with high probability, making it effective for dimensionality reduction.",
    "level": "basic"
  },
  {
    "id": 8,
    "question": "In the context of autoencoders, what is the “code”?",
    "options": [
      "The original input data.",
      "The output reconstruction.",
      "The hidden representation in a lower-dimensional space.",
      "The noise added to the input."
    ],
    "correct": [2],
    "explanation": "The 'code' in an autoencoder is the hidden representation in a lower-dimensional space.",
    "level": "basic"
  },
  {
    "id": 9,
    "question": "What is the primary goal of learning data representations?",
    "options": [
      "To predict specific labels",
      "To create human-interpretable results",
      "To transform data into vectors with desirable geometric properties",
      "To minimize model complexity"
    ],
    "correct": 2,
    "explanation": "Learning data representations transforms data into vectors that are separable, cluster by similarity, and useful for downstream tasks.",
    "level": "basic"
  },
  {
    "id": 10,
    "question": "What is a core motivation behind matrix factorization in recommender systems?",
    "options": [
      "Predicting labels directly",
      "Grouping data into archetypes",
      "Handling noisy data effectively",
      "Maximizing feature counts"
    ],
    "correct": 1,
    "explanation": "Matrix factorization often involves reducing data into archetypes, representing the key features in a lower-dimensional space.",
    "level": "basic"
  },
  
{
  "id": 11,
  "question": "What are some terms often used to mean a vector representation of a non-vector entity? (Multiple answers may apply)",
  "options": [
    "Embedding",
    "Representation",
    "Code",
    "Descriptor"
  ],
  "correct": [0, 1, 2, 3],
  "explanation": "Terms like 'embedding,' 'representation,' 'code,' and 'descriptor' are commonly used to refer to vector representations of non-vector entities, particularly in fields like natural language processing, computer vision, and machine learning.",
  "level": "basic"
}


  {
    "id": 12,
    "question": "Why are vector representations important in natural language processing?",
    "options": [
      "They improve tokenization performance",
      "They allow words with similar meanings to be mathematically close",
      "They reduce the number of features in the dataset",
      "They make models interpretable for humans"
    ],
    "correct": 1,
    "explanation": "Vector representations enable semantically similar words to have mathematically similar embeddings, improving downstream tasks like classification.",
    "level": "basic"
  },
  {
    "id": 13,
    "question": "What is the key benefit of using pre-trained embeddings in learning tasks?",
    "options": [
      "They reduce the size of the training dataset",
      "They encode domain knowledge and improve generalization",
      "They guarantee model convergence",
      "They make learning algorithms interpretable"
    ],
    "correct": 1,
    "explanation": "Pre-trained embeddings incorporate prior domain knowledge, improving generalization and reducing the need for large labeled datasets.",
    "level": "basic"
  },
  {
    "id": 14,
    "question": "What type of learning does Word2Vec leverage?",
    "options": [
      "Unsupervised learning",
      "Reinforcement learning",
      "Supervised learning",
      "Semi-supervised learning"
    ],
    "correct": 0,
    "explanation": "Word2Vec uses unsupervised learning to create embeddings by predicting context words or the target word based on surrounding words.",
    "level": "basic"
  },
  {
    "id": 15,
    "question": "Which of the following is a benefit of learning dense vector representations?",
    "options": [
      "They improve interpretability",
      "They minimize overfitting in high-dimensional spaces",
      "They make the representations sparse",
      "They are easier to visualize"
    ],
    "correct": 1,
    "explanation": "Dense vector representations minimize overfitting in high-dimensional spaces by efficiently encoding information.",
    "level": "basic"
  },
  {
    "id": 16,
    "question": "How can embeddings improve classification tasks?",
    "options": [
      "By replacing traditional feature engineering",
      "By introducing noise to the data",
      "By clustering the dataset into groups",
      "By increasing model complexity"
    ],
    "correct": 0,
    "explanation": "Embeddings act as learned features, replacing traditional feature engineering while improving classification performance.",
    "level": "basic"
  },
  {
    "id": 17,
    "question": "What is the main principle behind t-SNE as a visualization tool?",
    "options": [
      "Preserving exact distances in high-dimensional space",
      "Learning a mapping that clusters similar data in 2D or 3D",
      "Representing linear relationships between variables",
      "Reducing model complexity"
    ],
    "correct": 1,
    "explanation": "t-SNE maps high-dimensional data into a 2D or 3D space, preserving local relationships and clustering similar data points.",
    "level": "basic"
  },
  {
    "id": 18,
    "question": "What are some advantages of using vector representations for data? (Multiple answers may apply)",
    "options": [
      "They enable efficient similarity computations",
      "They are sparse by design",
      "They can generalize across tasks",
      "They inherently capture relationships between data points"
    ],
    "correct": [0, 2, 3],
    "explanation": "Vector representations are dense, enabling efficient similarity computations, generalizing across tasks, and capturing relationships between data points.",
    "level": "basic"
  },
  {
    "id": 19,
    "question": "What are the main benefits of pre-trained embeddings? (Multiple answers may apply)",
    "options": [
      "They reduce the need for large labeled datasets",
      "They eliminate the need for fine-tuning",
      "They encode useful domain knowledge",
      "They improve the interpretability of models"
    ],
    "correct": [0, 2],
    "explanation": "Pre-trained embeddings reduce the dependence on labeled datasets and encode domain knowledge, improving generalization.",
    "level": "basic"
  },
  {
    "id": 20,
    "question": "Which embedding technique is most suited for graph-based datasets?",
    "options": [
      "Word2Vec",
      "GloVe",
      "Node2Vec",
      "FastText"
    ],
    "correct": 2,
    "explanation": "Node2Vec is specifically designed for graph data, learning embeddings based on node relationships in the graph.",
    "level": "basic"
  },
  {
    "id": 21,
    "question": "What is a key limitation of static embeddings like Word2Vec?",
    "options": [
      "They require labeled datasets",
      "They cannot model polysemy (multiple meanings of a word)",
      "They are computationally expensive to train",
      "They are not differentiable"
    ],
    "correct": 1,
    "explanation": "Static embeddings assign a single vector per word, failing to capture multiple meanings of polysemous words.",
    "level": "basic"
  },
  {
    "id": 22,
    "question": "What are some characteristics of effective vector embeddings? (Multiple answers may apply)",
    "options": [
      "High-dimensional to encode all features",
      "Clusters semantically similar items",
      "Preserves relationships in the original data",
      "Can be used directly as input for machine learning models"
    ],
    "correct": [1, 2, 3],
    "explanation": "Effective embeddings cluster similar items, preserve relationships, and serve as direct input for machine learning models.",
    "level": "basic"
  },
  {
    "id": 23,
    "question": "Which concept is most associated with transfer learning in embeddings?",
    "options": [
      "Supervised learning",
      "Fine-tuning pre-trained models",
      "Feature engineering",
      "Dimensionality reduction"
    ],
    "correct": 1,
    "explanation": "Transfer learning involves fine-tuning pre-trained embeddings on specific tasks to leverage previously learned knowledge.",
    "level": "basic"
  },
  {
    "id": 24,
    "question": "What does the cosine similarity measure in vector spaces?",
    "options": [
      "The absolute difference between vectors",
      "The angular similarity between vectors",
      "The Euclidean distance between vectors",
      "The dot product of vectors"
    ],
    "correct": 1,
    "explanation": "Cosine similarity measures the angular relationship between two vectors, indicating their alignment or similarity.",
    "level": "basic"
  },
  {
    "id": 25,
    "question": "Why are embeddings particularly useful for high-dimensional categorical data?",
    "options": [
      "They reduce overfitting",
      "They transform categories into a compact continuous vector spaces",
      "They remove the need for supervised labels",
      "They are human-interpretable"
    ],
    "correct": 1,
    "explanation": "Embeddings map high-dimensional categorical data into a compact continuous vector spaces, enabling efficient modeling.",
    "level": "basic"
  },
  
  {
    "id": 26,
    "question": "Which properties make dense vector representations powerful in machine learning? (Multiple answers may apply)",
    "options": [
      "They are computationally efficient",
      "They can capture both linear and non-linear relationships",
      "They require no preprocessing",
      "They provide a compact representation of features"
    ],
    "correct": [0, 1, 3],
    "explanation": "Dense vectors are computationally efficient, compact, and capable of modeling both linear and non-linear relationships.",
    "level": "basic"
  },
  {
    "id": 27,
    "question": "Which tasks benefit directly from word embeddings? (Multiple answers may apply)",
    "options": [
      "Sentiment analysis",
      "Image recognition",
      "Text classification",
      "Machine translation"
    ],
    "correct": [0, 2, 3],
    "explanation": "Word embeddings are particularly useful in NLP tasks such as sentiment analysis, text classification, and machine translation.",
    "level": "basic"
  }, 
  {
    "id": 28,
    "question": "What methods can generate embeddings for words? (Multiple answers may apply)",
    "options": [
      "Word2Vec",
      "BERT",
      "Convolutional neural networks",
      "GloVe"
    ],
    "correct": [0, 1, 3],
    "explanation": "Word2Vec, BERT, and GloVe are popular methods for generating word embeddings. Convolutional networks are not typically used for this purpose.",
    "level": "basic"
  },
  {
    "id": 29,
    "question": "What are some challenges with learning embeddings? (Multiple answers may apply)",
    "options": [
      "Balancing dimensionality and information density",
      "Dealing with bias in the training data",
      "Capturing rare patterns or outliers",
      "Ensuring embeddings are sparse"
    ],
    "correct": [0, 1, 2],
    "explanation": "Challenges include balancing dimensionality, managing biases in training data, and capturing rare patterns or outliers effectively.",
    "level": "advanced"
  },
  {
    "id": 30,
    "question": "Which types of models commonly use learned vector embeddings? (Multiple answers may apply)",
    "options": [
      "Deep neural networks",
      "Support vector machines",
      "Graph neural networks",
      "Recurrent neural networks"
    ],
    "correct": [0, 2, 3],
    "explanation": "Deep neural networks, graph neural networks, and recurrent neural networks often use embeddings to represent data as input features.",
    "level": "advanced"
  },
  {
    "id": 31,
    "question": "What is the primary purpose of an autoencoder?",
    "options": [
      "To classify input data",
      "To reconstruct input data from a compressed representation",
      "To generate random noise",
      "To improve model interpretability"
    ],
    "correct": 1,
    "explanation": "An autoencoder is designed to learn a compressed representation of input data and reconstruct it back to the original format.",
    "level": "basic"
  },
  {
    "id": 32,
    "question": "What is the key difference between a variational autoencoder (VAE) and a standard autoencoder? (Multiple answers may apply)",
    "options": [
      "A VAE learns a probabilistic latent space",
      "A standard autoencoder does not reconstruct data",
      "A VAE includes a regularization term for latent variables",
      "A VAE uses supervised learning, while standard autoencoders are unsupervised"
    ],
    "correct": [0, 2],
    "explanation": "VAEs differ from standard autoencoders by learning a probabilistic latent space and introducing a regularization term to ensure the latent space follows a desired distribution (e.g., Gaussian).",
    "level": "advanced"
  },
  {
    "id": 33,
    "question": "What is the primary loss function used in a variational autoencoder?",
    "options": [
      "Mean squared error",
      "Cross-entropy loss",
      "Reconstruction loss combined with KL divergence",
      "Hinge loss"
    ],
    "correct": 2,
    "explanation": "A VAE uses a combination of reconstruction loss (to ensure accurate reconstruction) and KL divergence (to regularize the latent space).",
    "level": "advanced"
  },
  {
    "id": 34,
    "question": "In what scenarios are autoencoders typically used? (Multiple answers may apply)",
    "options": [
      "Dimensionality reduction",
      "Anomaly detection",
      "Feature extraction",
      "Supervised classification"
    ],
    "correct": [0, 1, 2],
    "explanation": "Autoencoders are commonly used for dimensionality reduction, anomaly detection, and feature extraction, but not for supervised classification as they are unsupervised models.",
    "level": "basic"
  },
  {
    "id": 35,
    "question": "What role does the encoder play in a variational autoencoder?",
    "options": [
      "It generates random noise",
      "It maps input data to a deterministic latent representation",
      "It maps input data to a probabilistic latent distribution",
      "It reconstructs input data"
    ],
    "correct": 2,
    "explanation": "In a VAE, the encoder maps input data to a probabilistic latent distribution (mean and variance), enabling sampling and generating diverse reconstructions.",
    "level": "advanced"
  }
]
